{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_authors = [3888979]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException ,TimeoutException,NoSuchWindowException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Spécifiez le chemin vers EdgeDriver et initialisez le navigateur Edge en utilisant ce service\n",
    "driver = webdriver.Edge(service=Service(executable_path=r\"C:\\Users\\Electro Fatal\\Desktop\\file\\edgedriver_win64\\msedgedriver.exe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "try:\n",
    "    # Accédez à la page de connexion\n",
    "    driver.get(\"https://www.webofscience.com.eressources.imist.ma/\")  # Remplacez par l'URL correcte\n",
    "\n",
    "    # Attendre que les champs email et mot de passe soient visibles\n",
    "    email_field = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.ID, \"email\"))\n",
    "    )\n",
    "    password_field = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.ID, \"password\"))\n",
    "    )\n",
    "\n",
    "    # Entrer les informations d'identification\n",
    "    email_field.send_keys('israa.boudda@usms.ac.ma')\n",
    "    password_field.send_keys('')\n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"Les champs d'email ou de mot de passe n'ont pas été trouvés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "def get_author_information(id):\n",
    "    driver.get(f\"https://www.webofscience.com.eressources.imist.ma/wos/author/record/{id}\")\n",
    "    # driver.get(f\"https://www.webofscience.com/wos/author/record/{id}\")\n",
    "    time.sleep(5)\n",
    "    scroll_slowly(driver)\n",
    "\n",
    "\n",
    "   # Vérifier si la page de l'auteur est introuvable\n",
    "    try:\n",
    "        if \"authorNotFound\" in driver.current_url:\n",
    "            print(f\"L'auteur avec l'ID {id} est introuvable.\")\n",
    "            return None, []  # Retourner None pour l'auteur introuvable\n",
    "    except NoSuchElementException:\n",
    "        pass  # Si l'élément d'erreur n'est pas trouvé, continuez normalement\n",
    "    try:\n",
    "        infos = {\n",
    "            'ID de l\\'Auteur' : id,\n",
    "            'nom_complet' : None,\n",
    "            'pays_affiliation' : None,\n",
    "            'co_auteurs' : 0,\n",
    "            'H-Index' : 0,\n",
    "            \"FWCI\": \"0\",\n",
    "            'Sum of Times Cited' : 0\n",
    "        }\n",
    "\n",
    "        # Récupération des co-auteurs\n",
    "        try:\n",
    "            co_auteurs = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, 'authors-list-link')))\n",
    "            infos['co_auteurs'] = [auteur.text for auteur in co_auteurs]\n",
    "            id_co_auteur = [auteur.get_attribute('href').split('/')[-1] for auteur in co_auteurs]\n",
    "        except:\n",
    "            infos['co_auteurs'] = []\n",
    "            id_co_auteur = []\n",
    "            print('Les co-auteurs n\\'existent pas')\n",
    "\n",
    "        # Récupération des métriques\n",
    "        try:\n",
    "            metric_descriptor = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, 'wat-author-metric-descriptor')))\n",
    "            for metric in metric_descriptor:\n",
    "                if metric.text in ['H-Index', 'Sum of Times Cited']:\n",
    "                    value = metric.find_element(By.XPATH, './preceding-sibling::div').text\n",
    "                    infos[metric.text] = value\n",
    "        except:\n",
    "            print('H-index ou bien citation n\\'existent pas')\n",
    "\n",
    "        # Récupération du pays d'affiliation\n",
    "        try:\n",
    "            value = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, 'more-details'))).text.split(',')[-1].strip()\n",
    "            infos['pays_affiliation'] = value\n",
    "        except:\n",
    "            print('Pays d\\'affiliation n\\'existe pas')\n",
    "\n",
    "        # Récupération du nom complet\n",
    "        try:\n",
    "            value = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, 'wat-author-name'))).text\n",
    "            infos['nom_complet'] = value\n",
    "        except:\n",
    "            print('Le nom n\\'existe pas')\n",
    "\n",
    "        return infos , id_co_auteur\n",
    "    except:\n",
    "        print('Erreur lors de l\\'attente du chargement de la page')\n",
    "\n",
    "def extract_article_details(driver, article_link):\n",
    "    driver.get(article_link)\n",
    "    time.sleep(4)  # Pause pour laisser le temps à la page de se charger\n",
    "    scroll_slowly(driver)  # Faire défiler la page lentement pour charger le contenu\n",
    "    infos = {}  # Dictionnaire pour stocker les informations de l'article\n",
    "\n",
    "\n",
    "    # Titre de l'article\n",
    "    try:\n",
    "        infos['Titre de l’article'] = wait.until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'title'))\n",
    "        ).text\n",
    "    except Exception as e:\n",
    "        infos['Titre de l’article'] = None\n",
    "        print(\"Erreur pour Titre de l’article:\")\n",
    "\n",
    "    # Auteur\n",
    "    try:\n",
    "        # Récupérer les noms des auteurs ou les mots-clés\n",
    "        authors = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//a[starts-with(@id,'SumAuthTa-DisplayName-author-en-')]\")))\n",
    "        infos['Auteurs'] = ' ; '.join([element.text for element in authors])  # Joindre les noms\n",
    "    except Exception as e:\n",
    "        infos['Auteur'] = None\n",
    "        print(\"Erreur pour Auteur:\" )\n",
    "\n",
    "    # Date de publication\n",
    "    class_names = ['FullRTa-pubdate', 'FullRTa-earlyAccess']\n",
    "    for class_name in class_names:\n",
    "        try:\n",
    "            infos['Date de publication'] = wait.until(EC.presence_of_element_located((By.ID, class_name))).text\n",
    "            break  # Sortir de la boucle si trouvé\n",
    "        except Exception as e:\n",
    "            infos['Date de publication'] = None\n",
    "            print(\"Erreur pour date:\" )\n",
    "\n",
    "    # Source\n",
    "    \"\"\"class_names = ['summary-source-title-link', 'summary-source-title']\n",
    "    for class_name in class_names:\n",
    "        try:\n",
    "            infos['Nom journal'] = wait.until(EC.presence_of_element_located((By.CLASS_NAME, class_name))).text\n",
    "            break  # Sortir de la boucle si trouvé\n",
    "        except Exception as e:\n",
    "            infos['nom journal'] = None\n",
    "            print(\"Erreur pour Source:\" )\"\"\"\n",
    "            # Source\n",
    "    class_names = ['summary-source-title-link', 'summary-source-title']\n",
    "    for class_name in class_names:\n",
    "        try:\n",
    "            journal_text = wait.until(EC.presence_of_element_located((By.CLASS_NAME, class_name))).text\n",
    "            infos['nom journal'] = journal_text.replace(\"arrow_drop_down\", \"\").strip()  # Supprimer \"arrow_drop_down\" et les espaces\n",
    "            break  # Sortir de la boucle si trouvé\n",
    "        except Exception as e:\n",
    "            infos['nom journal'] = None\n",
    "        print(\"Erreur pour Source:\")\n",
    "    # Mots-clés\n",
    "    mots_cles = ''\n",
    "    id_names = ['FRkeywordsTa-keyWordsPlusLink-','FRkeywordsTa-authorKeywordLink-']\n",
    "    for id_name in id_names:\n",
    "        try:\n",
    "            value = wait.until(EC.presence_of_all_elements_located((By.XPATH, f\"//a[starts-with(@id,'{id_name}')]\")))\n",
    "            mots_cles = mots_cles +' ' + ' ; '.join([element.text for element in value])  # Joindre les noms\n",
    "        except Exception as e:\n",
    "            infos['Mots-clés'] = None\n",
    "            print(\"Erreur pour Mots-clés:\" )\n",
    "\n",
    "    infos['Mots-clés'] = mots_cles\n",
    "\n",
    "    # Nombre de citations\n",
    "    try:\n",
    "        # Attendre et récupérer le nombre de citations\n",
    "        infoElem = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'citation-count'))).text.split('\\n')\n",
    "        if len(infoElem) > 1 and infoElem[1] == 'Cited References':\n",
    "            infos['Nombre de citations'] = 0  # Pas de citations\n",
    "        else:\n",
    "            infos['Nombre de citations'] = infoElem[0]  # Nombre de citations\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération de 'citation-count': {e}\")\n",
    "        infos['Nombre de citations'] = None  # Valeur par défaut en cas d'erreur\n",
    "\n",
    "    # DOI\n",
    "    try:\n",
    "        infos['DOI'] = wait.until(\n",
    "            EC.presence_of_element_located((By.ID, 'FullRTa-DOI'))\n",
    "        ).text\n",
    "    except Exception as e:\n",
    "        infos['DOI'] = None\n",
    "        print(\"Erreur pour DOI:\" )\n",
    "\n",
    "    # Résumé\n",
    "    try:\n",
    "        infos['Résumé'] = wait.until(\n",
    "            EC.presence_of_element_located((By.ID, 'FullRTa-abstract-basic'))\n",
    "        ).text\n",
    "    except Exception as e:\n",
    "        infos['Résumé'] = None\n",
    "        print(\"Erreur pour Résumé:\" )\n",
    "\n",
    "    # Type de document\n",
    "    try:\n",
    "        infos['Type de document'] = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"FullRTa-doctype-0\"]'))\n",
    "        ).text\n",
    "    except Exception as e:\n",
    "        infos['Type de document'] = None\n",
    "        print(\"Erreur pour Type de document:\" )\n",
    "\n",
    "    try:\n",
    "        # Récupérer l'ISSN de l'article\n",
    "        infos['issn'] = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'value.section-label-data.text-color'))).text\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        print(\"L'élément ISSN n'a pas été trouvé.\")\n",
    "\n",
    "    return infos\n",
    "\n",
    "\n",
    "# Fonction récupère les liens des articles en parcourant les pages d'une liste d'articles.\n",
    "def get_article_titles():\n",
    "    titles = []\n",
    "    while True:\n",
    "        articles = driver.find_elements(By.CLASS_NAME, 'title')\n",
    "        for article in articles:\n",
    "            titles.append(article.get_attribute('href'))\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH , '//button[@data-ta=\"next-page-button\"]')\n",
    "            if 'mat-button-disabled' in next_button.get_attribute('class'):\n",
    "                break\n",
    "            else:\n",
    "                next_button.click()\n",
    "                scroll_slowly(driver, scroll_pause_time=0.2, scroll_increment=100)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    return titles\n",
    "\n",
    "# Fonction qui fait défiler lentement la page dans le navigateur.\n",
    "def scroll_slowly(driver, scroll_pause_time=0.1, scroll_increment=100):\n",
    "\n",
    "    # Récupère la hauteur totale de la page.\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    current_scroll_position = 0  # Commence en haut de la page\n",
    "\n",
    "    while current_scroll_position < last_height:\n",
    "        # Fait défiler la page par petits incréments (scroll_increment)\n",
    "        current_scroll_position += scroll_increment\n",
    "        driver.execute_script(f\"window.scrollTo(0, {current_scroll_position});\")\n",
    "\n",
    "        # Attend un court instant entre les défilements\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Met à jour la hauteur totale au cas où la page chargerait dynamiquement plus de contenu\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Search function for journal information based on ISSN\n",
    "def Search_journal_info(driver, issn):\n",
    "    driver.get(\"https://www.scimagojr.com/\")\n",
    "    try:\n",
    "        search_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"searchinput\"))\n",
    "        )\n",
    "        search_input.clear()\n",
    "        search_input.send_keys(issn, Keys.RETURN)\n",
    "\n",
    "        # Wait and click on the search result\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"search_results\"))\n",
    "        ).click()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Extraction uniquement du nom du journal\n",
    "# Extract publisher and journal name from the page source\n",
    "def extract_publisher_and_journal_name(soup):\n",
    "    publisher = None\n",
    "    # Find the publisher by searching for a specific tag structure\n",
    "    publisher_element = soup.find('a', href=lambda href: href and \"journalsearch.php?q=\" in href)\n",
    "    if publisher_element:\n",
    "        publisher = publisher_element.text.strip()\n",
    "\n",
    "    return {\"publisher\": publisher}\n",
    "# Continue with the existing functions for extracting metrics\n",
    "def extract_quartile(soup):\n",
    "    quartile_data = {}\n",
    "    dash = soup.findAll('div', class_='dashboard')\n",
    "    if dash:\n",
    "        quart_dash = dash[0].findAll('div', class_=\"cellslide\")\n",
    "        if quart_dash:\n",
    "            last_quartile_row = quart_dash[1].find('tbody').find_all('tr')[-1].find_all('td')\n",
    "            if len(last_quartile_row) == 3:\n",
    "                quartile_data = {\n",
    "                    \"year\": last_quartile_row[1].text,\n",
    "                    \"quartile_value\": last_quartile_row[2].text\n",
    "                }\n",
    "    return quartile_data\n",
    "\n",
    "def extract_sjr(soup):\n",
    "    sjr_data = {}\n",
    "    dash = soup.findAll('div', class_='dashboard')\n",
    "    if len(dash) > 1:\n",
    "        sjr_dash = dash[1].findAll('div', class_=\"cellslide\")[1]\n",
    "        sjr_row = sjr_dash.find('tbody').find_all('tr')[-1].find_all('td')\n",
    "        if len(sjr_row) == 2:\n",
    "            sjr_data = {\n",
    "                \"year\": sjr_row[0].text,\n",
    "                \"sjr_value\": sjr_row[1].text\n",
    "            }\n",
    "    return sjr_data\n",
    "\n",
    "def extract_impact_factor(soup):\n",
    "    impact_data = {}\n",
    "    dash = soup.findAll('div', class_='dashboard')\n",
    "    if len(dash) > 1:\n",
    "        impact_dash = dash[1].findAll('div', class_=\"cellslide\")[5]\n",
    "        impact_row = impact_dash.find('tbody').find_all('tr')[-1].find_all('td')\n",
    "        if len(impact_row) == 3:\n",
    "            impact_data = {\n",
    "                \"year\": impact_row[1].text,\n",
    "                \"impact_factor_value\": impact_row[2].text\n",
    "            }\n",
    "    return impact_data\n",
    "\n",
    "# Main function to gather journal information\n",
    "def extract_journal_metrics(driver, issn_list):\n",
    "    journals_info = []\n",
    "\n",
    "    for issn in issn_list:\n",
    "        try:\n",
    "            Search_journal_info(driver, issn)\n",
    "\n",
    "            # Extract page content with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Extract journal metrics\n",
    "            journal_data = {\n",
    "                \"issn\": issn,\n",
    "                \"h_index\": WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"hindexnumber\"))\n",
    "                ).text if WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"hindexnumber\"))) else \"N/A\",\n",
    "                \"scope\": WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"fullwidth\"))\n",
    "                ).text.split(\"\\n\")[1] if WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"fullwidth\"))) else \"N/A\",\n",
    "                \"quartile\": extract_quartile(soup),\n",
    "                \"sjr\": extract_sjr(soup),\n",
    "                \"impact_factor\": extract_impact_factor(soup)\n",
    "            }\n",
    "\n",
    "            # Add publisher and journal name\n",
    "            journal_info = extract_publisher_and_journal_name(soup)\n",
    "            journal_data.update(journal_info)\n",
    "\n",
    "            journals_info.append(journal_data)\n",
    "            print(\"Retrieved info for ISSN:\", issn)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing ISSN {issn}: {e}\")\n",
    "            journals_info.append({\n",
    "                \"issn\": issn,\n",
    "                \"h_index\": \"N/A\",\n",
    "                \"scope\": \"N/A\",\n",
    "                \"quartile\": \"N/A\",\n",
    "                \"sjr\": {\"sjr_value\": \"N/A\", \"year\": \"N/A\"},\n",
    "                \"impact_factor\": {\"impact_factor_value\": \"N/A\", \"year\": \"N/A\"},\n",
    "                \"publisher\": \"N/A\"\n",
    "            })\n",
    "\n",
    "    return journals_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []  # Initialiser une liste pour stocker chaque auteur et co-auteur sans clé ID\n",
    "processed_authors = set()  # Utiliser un set pour éviter les doublons d'auteurs\n",
    "\n",
    "def fetch_author_data_with_articles(author_id):\n",
    "    \"\"\"Récupère les informations de l'auteur et ses articles.\"\"\"\n",
    "    # Récupérer les informations de l'auteur principal et les IDs de co-auteurs\n",
    "    author_data, co_authors_ids = get_author_information(author_id)\n",
    "    author_data['Articles'] = fetch_articles_with_journal_data(author_id)\n",
    "\n",
    "    # Ajouter l'auteur principal à all_data si non déjà traité\n",
    "    if author_id not in processed_authors:\n",
    "        all_data.append(author_data)\n",
    "        processed_authors.add(author_id)\n",
    "\n",
    "    # Traiter les co-auteurs\n",
    "    for co_author_id in co_authors_ids:\n",
    "        if co_author_id not in processed_authors:\n",
    "            co_author_data, _ = get_author_information(co_author_id)\n",
    "            co_author_data['Articles'] = fetch_articles_with_journal_data(co_author_id)\n",
    "            all_data.append(co_author_data)  # Ajouter le co-auteur\n",
    "            processed_authors.add(co_author_id)\n",
    "\n",
    "def fetch_articles_with_journal_data(author_id=None):\n",
    "    \"\"\"Récupère les articles et leurs données de journal pour un auteur donné.\"\"\"\n",
    "    all_articles_data = []\n",
    "\n",
    "    # Obtenir les titres des articles\n",
    "    while True:\n",
    "        articles = get_article_titles()\n",
    "        if articles:\n",
    "            break\n",
    "        driver.refresh()\n",
    "        time.sleep(1)\n",
    "        scroll_slowly(driver)\n",
    "\n",
    "    if articles:\n",
    "        issns = []\n",
    "        for article_link in articles:\n",
    "            data_article = fetch_article_data(article_link)\n",
    "            all_articles_data.append(data_article)\n",
    "            issns.append(data_article.get('issn'))\n",
    "\n",
    "        # Extraire les données des journaux associés en utilisant les ISSNs des articles\n",
    "        journals_data = extract_journal_metrics(driver, issns)\n",
    "        for article, journal in zip(all_articles_data, journals_data):\n",
    "            if article.get('issn') == journal.get('issn'):\n",
    "                article['journal_data'] = journal\n",
    "            article.pop('issn', None)  # Supprimer l'ISSN après association\n",
    "\n",
    "    return all_articles_data\n",
    "\n",
    "def fetch_article_data(article_link):\n",
    "    \"\"\"Extrait les détails de l'article, avec gestion des cas d'absence d'ISSN.\"\"\"\n",
    "    flag = 0\n",
    "    while True:\n",
    "        data_article = extract_article_details(driver, article_link)\n",
    "        if 'issn' in data_article:\n",
    "            return data_article\n",
    "        else:\n",
    "            flag += 1\n",
    "            scroll_slowly(driver)\n",
    "            if flag >= 1:\n",
    "                data_article['issn'] = None\n",
    "                return data_article\n",
    "\n",
    "def main():\n",
    "    for author_id in list_authors:\n",
    "        if author_id not in processed_authors:\n",
    "            fetch_author_data_with_articles(author_id)  # Ajouter auteur et co-auteurs\n",
    "\n",
    "    # Enregistrer dans un fichier JSON\n",
    "    with open(\"testo.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(all_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Données enregistrées dans le fichier 'testo.json'.\")\n",
    "    return all_data\n",
    "\n",
    "# Exécution principale\n",
    "try:\n",
    "    all_data_result = main()\n",
    "except NoSuchWindowException:\n",
    "    print(\"La fenêtre du navigateur a été fermée. Redémarrage de la session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_data.pop('63939275'))\n",
    "print(len(all_data))\n",
    "# (id_co_auteurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Supposons que 'all_data_result' soit le résultat final de la fonction 'main()' qui contient toutes les données.\n",
    "all_data_result = main()\n",
    "\n",
    "# Enregistrer 'all_data_result' dans un fichier JSON\n",
    "with open(\"Scrap3.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(all_data_result, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
